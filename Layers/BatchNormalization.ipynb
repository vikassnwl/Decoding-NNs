{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a technique in neural networks to improve training speed and stability. It normalizes the inputs to a layer within a mini-batch, ensuring that they have a mean of 0 and a standard deviation of 1. After normalization, the technique introduces two learnable parameters, a scaling factor ($ \\gamma $) and a shift factor ($ \\beta $), to allow the network to learn the optimal scale and offset for each feature.\n",
    "\n",
    "### Benefits:\n",
    "1. **Improved Training Speed**: It allows higher learning rates by reducing the risk of exploding or vanishing gradients.\n",
    "2. **Regularization Effect**: It adds slight noise to the training process due to mini-batch calculations, which can reduce overfitting.\n",
    "3. **Reduced Sensitivity to Initialization**: It alleviates the need for careful weight initialization.\n",
    "\n",
    "### Formula:\n",
    "For a mini-batch $ x = \\{x_1, x_2, \\dots, x_m\\} $:\n",
    "1. Compute batch mean: $ \\mu = \\frac{1}{m} \\sum_{i=1}^m x_i $\n",
    "2. Compute batch variance: $ \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2 $\n",
    "3. Normalize: $ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $\n",
    "4. Scale and shift: $ y_i = \\gamma \\hat{x}_i + \\beta $\n",
    "\n",
    "Here, $ \\epsilon $ is a small constant to avoid division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why batch_normalization has 128 parameters?\n",
    "```\n",
    "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv2d (Conv2D)                 │ (None, 32, 32, 32)     │           896 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ batch_normalization             │ (None, 32, 32, 32)     │           128 │\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 128 parameters in the `batch_normalization` layer represent the learnable parameters of the layer: **scale ($ \\gamma $)** and **shift ($ \\beta $)**. These parameters allow the model to adjust the normalized values during training.\n",
    "\n",
    "### Explanation:\n",
    "1. **Input Shape to Batch Normalization**: `(None, 32, 32, 32)`\n",
    "   - The last dimension, `32`, represents the number of channels (or filters) from the preceding `Conv2D` layer.\n",
    "\n",
    "2. **Learnable Parameters**:\n",
    "   - For each channel, batch normalization learns:\n",
    "     - $ \\gamma $ (scale parameter): 32 parameters\n",
    "     - $ \\beta $ (shift parameter): 32 parameters\n",
    "   - Total learnable parameters = \\( 32 + 32 = 64 \\).\n",
    "\n",
    "3. **Non-learnable Parameters**:\n",
    "   - Batch normalization also maintains two moving statistics for each channel:\n",
    "     - **Moving mean**: 32 values\n",
    "     - **Moving variance**: 32 values\n",
    "   - These are not updated by backpropagation but are used during inference.\n",
    "   - Total non-learnable parameters = \\( 32 + 32 = 64 \\).\n",
    "\n",
    "### Total Parameters:\n",
    "- Learnable parameters: \\( 64 \\)\n",
    "- Non-learnable parameters: \\( 64 \\)\n",
    "- **Total parameters: \\( 64 + 64 = 128 \\)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you elaborate on moving mean and variance?\n",
    "The **moving mean** and **moving variance** in batch normalization are non-learnable parameters used to keep track of the mean and variance of the activations during training. They play a critical role during **inference (testing)** when batch statistics are no longer computed dynamically.\n",
    "\n",
    "### What are moving mean and moving variance?\n",
    "1. **Moving Mean ($ \\mu_{\\text{moving}} $)**:\n",
    "   - It is an estimate of the mean of the layer's activations across the entire dataset, computed and updated during training.\n",
    "   - During **training**, for each mini-batch, the mean of the activations ($ \\mu_{\\text{batch}} $) is calculated. This batch mean is then used to update the moving mean:\n",
    "     $$\n",
    "     \\mu_{\\text{moving}} = (1 - \\text{momentum}) \\cdot \\mu_{\\text{batch}} + \\text{momentum} \\cdot \\mu_{\\text{moving}}\n",
    "     $$\n",
    "   - The `momentum` parameter (default ~0.99 or 0.9) controls how much of the previous moving mean is retained.\n",
    "\n",
    "2. **Moving Variance ($ \\sigma_{\\text{moving}}^2 $)**:\n",
    "   - Similar to the moving mean, this tracks the variance of the activations across the dataset.\n",
    "   - It is updated using the batch variance ($ \\sigma_{\\text{batch}}^2 $) computed for each mini-batch:\n",
    "     $$\n",
    "     \\sigma_{\\text{moving}}^2 = (1 - \\text{momentum}) \\cdot \\sigma_{\\text{batch}}^2 + \\text{momentum} \\cdot \\sigma_{\\text{moving}}^2\n",
    "     $$\n",
    "\n",
    "### Purpose:\n",
    "- During **training**, the mini-batch mean and variance are used to normalize the activations.\n",
    "- During **inference**, the moving mean and moving variance are used instead, since the test data may not be passed in mini-batches, and computing statistics dynamically is infeasible.\n",
    "\n",
    "### Key Points:\n",
    "- The moving mean and variance are **not updated by backpropagation**; they are updated directly during forward passes in training.\n",
    "- They do not count as learnable parameters since the model does not optimize them during training.\n",
    "\n",
    "### Practical Example:\n",
    "- Suppose during training, in the first few mini-batches, the mean and variance of a channel's activations are as follows:\n",
    "  - Batch 1: $ \\mu_{\\text{batch}} = 0.5 $, $ \\sigma_{\\text{batch}}^2 = 0.25 $\n",
    "  - Batch 2: $ \\mu_{\\text{batch}} = 0.6 $, $ \\sigma_{\\text{batch}}^2 = 0.2 $\n",
    "  - With a momentum of 0.9, the moving estimates are updated as:\n",
    "    $$\n",
    "    \\mu_{\\text{moving}} = 0.9 \\cdot \\mu_{\\text{moving (prev)}} + 0.1 \\cdot \\mu_{\\text{batch}}\n",
    "    $$\n",
    "    $$\n",
    "    \\sigma_{\\text{moving}}^2 = 0.9 \\cdot \\sigma_{\\text{moving (prev)}}^2 + 0.1 \\cdot \\sigma_{\\text{batch}}^2\n",
    "    $$\n",
    "\n",
    "By the end of training, the moving mean and variance provide a stable estimate of the activation statistics for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy images for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAGbCAYAAABETtCOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdPUlEQVR4nO3df0yVdf/H8dcxBEyQ+QtNUUmokUBWrDUzw0TJMi2HUstmSnWzFGvet2mtpvXVQm0zUfNHeadZ3msdze24WFuZGDnL7mUMf+UvtES96yhSqLmUz/ePxskT+AZ/cQifj83t5jrXxfW5Lu3zPNd1ftwe55wTAACoU4tQDwAAgKaMUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgKFRQ/nyyy/L4/HI7/c35m7Pq6ioSB6PR6tWrQrJ/mvOBwAbc0cw5o7G9be/oly4cKGWL18ekn1XVVVp2rRpGjx4sNq1ayePxxOysTRVhw8f1vPPP6977rlH0dHR8ng8KioqCvWwAOaOJm7dunXKycnRjTfeqGuvvVY9e/bUk08+qcOHDzf6WAjlJfD7/fq///s/7dixQ717977g7V966SWdOnXqCoys6fj+++81a9YslZeXKzU1NdTDAQKYO5q2KVOmqKioSMOHD9e8efP0yCOP6MMPP9Stt96qI0eONOpYwhp1b83Mddddp8OHD6tz587673//q9tvv/2Ctg8LC1NYWPP+K0hLS9PRo0fVrl07rVq1SiNHjgz1kICQY+6o35w5c3TXXXepRYs/r+cGDx6s9PR0LViwQDNmzGi0sYTkitLv9ys7O1tt2rRR+/bt9eyzz+q3334LWmfZsmUaMGCAYmNjFRERoV69emnRokVB68THx2vbtm3asGGDPB6PPB6P+vfvH3j8+PHjmjhxouLj4xUREaG4uDiNHj261usc1dXVevXVVxUXF6fIyEhlZGRoz5499R5HRESEOnfufNHnoa7XGTwej/Ly8uT1etWrVy+1atVKffr0UWlpqSRpyZIlSkxMVGRkpPr376/9+/cHbV9cXKyRI0eqe/fuioiIULdu3TRx4sQ6n33W7CMyMlIpKSlas2aNxowZo/j4+KD1qqurNXfuXCUnJysyMlKdOnVSbm6uKioq6j3G6OhotWvX7sJODHAezB1/uBrmjrvvvjsokjXL2rVrpx07djTgLF0+IXlKkp2drfj4eOXn5+urr77SvHnzVFFRoRUrVgTWWbRokZKTkzVs2DCFhYVp7dq1GjdunKqrqzV+/HhJ0ty5czVhwgRFRUXpxRdflCR16tRJ0h+vAfTr1087duxQTk6ObrvtNvn9fvl8Ph08eFAdOnQI7GvmzJlq0aKFJk2apMrKSs2ePVujRo3S119/3Yhn5U/FxcXy+XyB48zPz9cDDzygyZMna+HChRo3bpwqKio0e/Zs5eTk6PPPPw9s6/V6dfLkST399NNq3769Nm/erPnz5+vgwYPyer2B9T7++GM9/PDDSk1NVX5+vioqKvTEE0+oa9eutcaTm5ur5cuXa+zYsXrmmWdUVlamBQsWaMuWLdq4caNatmx55U8KIOaO+jT3uaOqqkpVVVVBfweNwjWiadOmOUlu2LBhQcvHjRvnJLmSkpLAspMnT9ba/t5773U9e/YMWpacnOzS09NrrTt16lQnyX300Ue1HquurnbOObd+/Xonyd10003u9OnTgccLCgqcJFdaWtrgY/vmm2+cJLds2bIGb1NzPs4lyUVERLiysrLAsiVLljhJrnPnzu6XX34JLH/hhRecpKB16zpv+fn5zuPxuAMHDgSWpaamuri4OPfrr78GlhUVFTlJrkePHoFlxcXFTpJbuXJl0O/85JNP6lxu8Xq9TpJbv359g7cBnGPu+Kurbe6oMX36dCfJrVu37oK3vRQhufVa82ynxoQJEyRJhYWFgWWtWrUK/O/Kykr5/X6lp6dr3759qqysrHcfq1evVu/evTV8+PBaj/31lsXYsWMVHh4e+Llfv36SpH379jXgaC6/jIyMoFsYd9xxhyQpKytL0dHRtZafO85zz9uJEyfk9/t15513yjmnLVu2SJIOHTqk0tJSjR49WlFRUYH109PTa73hxuv1KiYmRoMGDZLf7w/8SUtLU1RUlNavX3/5DhyoB3OHrTnPHV988YVeeeUVZWdna8CAARe07aUKya3XG264IejnhIQEtWjRIuie+caNGzVt2jRt2rRJJ0+eDFq/srJSMTEx5j727t2rrKysBo2ne/fuQT+3bdtWkhp0H/1K+Ot4ao61W7dudS4/d5w//PCDpk6dKp/PV2v8NZPEgQMHJEmJiYm19p2YmKhvv/028PPu3btVWVmp2NjYOsf6008/NeiYgMuBucPWXOeOnTt3avjw4UpJSdHSpUsbvN3l0iTeNvXXZ2l79+5VRkaGkpKSNGfOHHXr1k3h4eEqLCzUG2+8oerq6su6/2uuuabO5c65y7qfhjrfeOob59mzZzVo0CAdO3ZMU6ZMUVJSklq3bq3y8nKNGTPmos5bdXW1YmNjtXLlyjof79ix4wX/TuByYe4I1hznjh9//FGZmZmKiYlRYWFh0JVxYwlJKHfv3q3rr78+8POePXtUXV0duGWwdu1anT59Wj6fL+gZUl2X6uf7doqEhARt3br18g68iSstLdWuXbv07rvvavTo0YHln376adB6PXr0kKQ6353312UJCQn67LPP1Ldv36BbM0AoMHdcGU117jh69KgyMzN1+vRprVu3Ttddd91F/Z5LFZLXKN98882gn+fPny9Juu+++yT9+ezn3GdllZWVWrZsWa3f1bp1ax0/frzW8qysLJWUlGjNmjW1HgvVs70rra7z5pxTQUFB0HpdunRRSkqKVqxYoaqqqsDyDRs2BN5KXiM7O1tnz57V9OnTa+3vzJkzdZ574Eph7rgymuLcceLECd1///0qLy9XYWFhrdvujSkkV5RlZWUaNmyYBg8erE2bNun999/Xo48+GviGiszMTIWHh2vo0KHKzc1VVVWV3n77bcXGxtb6+qK0tDQtWrRIM2bMUGJiomJjYzVgwAA999xzgQ+45+TkKC0tTceOHZPP59PixYsv6tsw6rJgwQIdP35chw4dkvTHM9qDBw9K+uONBvW9HnI5JSUlKSEhQZMmTVJ5ebnatGmj1atX1/l6yWuvvaYHH3xQffv21dixY1VRUaEFCxYoJSUl6D+A9PR05ebmKj8/X999950yMzPVsmVL7d69W16vVwUFBRoxYoQ5rpoPBm/btk2S9N577+nLL7+U9Mc3jAANxdxxZTTFuWPUqFHavHmzcnJytGPHjqDPTkZFRemhhx66rOfA1Jhvsa15S/P27dvdiBEjXHR0tGvbtq3Ly8tzp06dClrX5/O5m2++2UVGRrr4+Hg3a9Ys984779R6S/ORI0fckCFDXHR0tJMU9Hbvo0ePury8PNe1a1cXHh7u4uLi3OOPP+78fr9z7s+3eHu93qB9l5WVNfjt2j169HCS6vxz7jit83EuSW78+PF1juf1118PWl7X+Ldv3+4GDhzooqKiXIcOHdxTTz3lSkpK6jyeDz74wCUlJbmIiAiXkpLifD6fy8rKcklJSbXG+tZbb7m0tDTXqlUrFx0d7VJTU93kyZPdoUOH6j1H5zs/jfzPD39jzB11n49zNbe5wzo/534MpTF4nGum9xJwUW655RZ17Nix1msTAGBpznPH3/5L0XFxfv/9d505cyZoWVFRkUpKSoK+ygsAznU1zh1cUV6l9u/fr4EDB+qxxx5Tly5dtHPnTi1evFgxMTHaunWr2rdvH+ohAmiCrsa5o0l8jhKNr23btkpLS9PSpUv1888/q3Xr1hoyZIhmzpzZLP+hA7g8rsa5gytKAAAMvEYJAICBUAIAYGjwa5S5ublXchxXnaFDh4Z6CM3KAw88EOoh4DzGjRsX6iE0K6H6fyZprj755JN61+GKEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwhDV0xZMnT17JcVx1Ro0aFeohNCuVlZWhHgLOIysrK9RDaFYeeeSRUA/hqsMVJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgIJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgIJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgIJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgIJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgIJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgIJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBg8DjnXKgHAQBAU8UVJQAABkIJAICBUAIAYCCUAAAYCCUAAAZCCQCAgVACAGAglAAAGAglAAAGQgkAgIFQAgBgIJQAABgaNZQvv/yyPB6P/H5/Y+72vIqKiuTxeLRq1aqQ7L/mfACwMXcEY+5oXH/7K8qFCxdq+fLlIdn3N998o7y8PCUnJ6t169bq3r27srOztWvXrpCMpyk6fPiwnn/+ed1zzz2Kjo6Wx+NRUVFRqIcFMHc0cevWrVNOTo5uvPFGXXvtterZs6eefPJJHT58uNHHQigvwaxZs7R69WplZGSooKBA//jHP/TFF1/otttu09atW+vd/qWXXtKpU6caYaSh8/3332vWrFkqLy9XampqqIcDBDB3NG1TpkxRUVGRhg8frnnz5umRRx7Rhx9+qFtvvVVHjhxp1LGENerempl//vOf+s9//qPw8PDAsocfflipqamaOXOm3n//fXP7sLAwhYU177+CtLQ0HT16VO3atdOqVas0cuTIUA8JCDnmjvrNmTNHd911l1q0+PN6bvDgwUpPT9eCBQs0Y8aMRhtLSK4o/X6/srOz1aZNG7Vv317PPvusfvvtt6B1li1bpgEDBig2NlYRERHq1auXFi1aFLROfHy8tm3bpg0bNsjj8cjj8ah///6Bx48fP66JEycqPj5eERERiouL0+jRo2u9zlFdXa1XX31VcXFxioyMVEZGhvbs2VPvcdx5551B/9Al6YYbblBycrJ27NhR7/Z1vc7g8XiUl5cnr9erXr16qVWrVurTp49KS0slSUuWLFFiYqIiIyPVv39/7d+/P2j74uJijRw5Ut27d1dERIS6deumiRMn1vnss2YfkZGRSklJ0Zo1azRmzBjFx8fXOj9z585VcnKyIiMj1alTJ+Xm5qqioqLeY4yOjla7du3qXQ9oCOaOP1wNc8fdd98dFMmaZe3atWvQObqcQvKUJDs7W/Hx8crPz9dXX32lefPmqaKiQitWrAiss2jRIiUnJ2vYsGEKCwvT2rVrNW7cOFVXV2v8+PGSpLlz52rChAmKiorSiy++KEnq1KmTJKmqqkr9+vXTjh07lJOTo9tuu01+v18+n08HDx5Uhw4dAvuaOXOmWrRooUmTJqmyslKzZ8/WqFGj9PXXX1/wsTnn9L///U/JyckXfX6Ki4vl8/kCx5mfn68HHnhAkydP1sKFCzVu3DhVVFRo9uzZysnJ0eeffx7Y1uv16uTJk3r66afVvn17bd68WfPnz9fBgwfl9XoD63388ceBZ7D5+fmqqKjQE088oa5du9YaT25urpYvX66xY8fqmWeeUVlZmRYsWKAtW7Zo48aNatmy5UUfK3AhmDtszX3uqKqqUlVVVdDfQaNwjWjatGlOkhs2bFjQ8nHjxjlJrqSkJLDs5MmTtba/9957Xc+ePYOWJScnu/T09FrrTp061UlyH330Ua3HqqurnXPOrV+/3klyN910kzt9+nTg8YKCAifJlZaWXtDxOefce++95yS5f//73/WuW3M+ziXJRUREuLKyssCyJUuWOEmuc+fO7pdffgksf+GFF5ykoHXrOm/5+fnO4/G4AwcOBJalpqa6uLg49+uvvwaWFRUVOUmuR48egWXFxcVOklu5cmXQ7/zkk0/qXG7xer1Oklu/fn2DtwGcY+74q6tt7qgxffp0J8mtW7fugre9FCG59VrzbKfGhAkTJEmFhYWBZa1atQr878rKSvn9fqWnp2vfvn2qrKysdx+rV69W7969NXz48FqP/fWWxdixY4Nug/Tr10+StG/fvgYczZ927typ8ePHq0+fPnr88ccvaNtzZWRkBN3CuOOOOyRJWVlZio6OrrX83HGee95OnDghv9+vO++8U845bdmyRZJ06NAhlZaWavTo0YqKigqsn56eXusNN16vVzExMRo0aJD8fn/gT1pamqKiorR+/fqLPk7gQjF32Jrz3PHFF1/olVdeUXZ2tgYMGHBB216qkNx6veGGG4J+TkhIUIsWLYLumW/cuFHTpk3Tpk2bdPLkyaD1KysrFRMTY+5j7969ysrKatB4unfvHvRz27ZtJalB99FrHDlyREOGDFFMTIxWrVqla665psHb1jeemmPt1q1bncvPHecPP/ygqVOnyufz1Rp/zSRx4MABSVJiYmKtfScmJurbb78N/Lx7925VVlYqNja2zrH+9NNPDTom4HJg7riw8TSXuWPnzp0aPny4UlJStHTp0gZvd7k0ibdN/fVZ2t69e5WRkaGkpCTNmTNH3bp1U3h4uAoLC/XGG2+ourr6su7/fP8wnXMN2r6yslL33Xefjh8/ruLiYnXp0uWKjKe+cZ49e1aDBg3SsWPHNGXKFCUlJal169YqLy/XmDFjLuq8VVdXKzY2VitXrqzz8Y4dO17w7wQuF+aOho3n7zx3/Pjjj8rMzFRMTIwKCwuDrowbS0hCuXv3bl1//fWBn/fs2aPq6urALYO1a9fq9OnT8vl8Qc+Q6rpUP9+3UyQkJDTo80iX6rffftPQoUO1a9cuffbZZ+rVq9cV3+f5lJaWateuXXr33Xc1evTowPJPP/00aL0ePXpIUp3vzvvrsoSEBH322Wfq27dv0K0ZIBSYO66Mpjp3HD16VJmZmTp9+rTWrVun66677qJ+z6UKyWuUb775ZtDP8+fPlyTdd999kv589nPus7LKykotW7as1u9q3bq1jh8/Xmt5VlaWSkpKtGbNmlqPNfTZXn3Onj2rhx9+WJs2bZLX61WfPn0uy++9WHWdN+ecCgoKgtbr0qWLUlJStGLFClVVVQWWb9iwIfBW8hrZ2dk6e/aspk+fXmt/Z86cqfPcA1cKc8eV0RTnjhMnTuj+++9XeXm5CgsLa912b0whuaIsKyvTsGHDNHjwYG3atEnvv/++Hn30UfXu3VuSlJmZqfDwcA0dOlS5ubmqqqrS22+/rdjY2FpfX5SWlqZFixZpxowZSkxMVGxsrAYMGKDnnnsu8AH3nJwcpaWl6dixY/L5fFq8eHFgX5fiX//6l3w+n4YOHapjx47V+pDwY489dsn7uBBJSUlKSEjQpEmTVF5erjZt2mj16tV1vl7y2muv6cEHH1Tfvn01duxYVVRUaMGCBUpJSQn6DyA9PV25ubnKz8/Xd999p8zMTLVs2VK7d++W1+tVQUGBRowYYY6r5oPB27ZtkyS99957+vLLLyX98Q0jQEMxd1wZTXHuGDVqlDZv3qycnBzt2LEj6LOTUVFReuihhy7rOTA15ltsa97SvH37djdixAgXHR3t2rZt6/Ly8typU6eC1vX5fO7mm292kZGRLj4+3s2aNcu98847td7SfOTIETdkyBAXHR3tJAW93fvo0aMuLy/Pde3a1YWHh7u4uDj3+OOPO7/f75z78y3eXq83aN9lZWVOklu2bJl5POnp6U7Sef809HycS5IbP358neN5/fXXg5bXNf7t27e7gQMHuqioKNehQwf31FNPuZKSkjqP54MPPnBJSUkuIiLCpaSkOJ/P57KyslxSUlKtsb711lsuLS3NtWrVykVHR7vU1FQ3efJkd+jQoXqP81LOEeAcc8f5zse5mtvc0aNHj/Oen3M/htIYPM5dpnsJaBZuueUWdezYsdZrEwBgac5zx9/+S9FxcX7//XedOXMmaFlRUZFKSkqCvsoLAM51Nc4dXFFepfbv36+BAwfqscceU5cuXbRz504tXrxYMTEx2rp1q9q3bx/qIQJogq7GuaNJfI4Sja9t27ZKS0vT0qVL9fPPP6t169YaMmSIZs6c2Sz/oQO4PK7GuYMrSgAADLxGCQCAgVACAGBo8GuUubm5V3IcV53mei8/lF577bVQDwF1uP/++0M9hGbllVdeCfUQmpXbb7+93nW4ogQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADGENXTEvL+9KjuOqExbW4FMP/K21adMm1ENoVj788MNQD6FZuf322+tdhytKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMBAKAEAMBBKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAyEEgAAA6EEAMDgcc65UA8CAICmiitKAAAMhBIAAAOhBADAQCgBADAQSgAADIQSAAADoQQAwEAoAQAwEEoAAAz/DyJBctu6HhFaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "batch_1 = np.random.randint(0, 255, (2, 2, 2, 1))  # batch 1 having 2 images\n",
    "batch_2 = np.random.randint(0, 255, (2, 2, 2, 1))  # batch 2 having 2 images\n",
    "batches = [batch_1, batch_2]\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "for i, batch in enumerate(batches):\n",
    "    for j, image in enumerate(batch):\n",
    "        ax = axes[i][j]\n",
    "        ax.imshow(image, cmap=\"gray\", vmin=0, vmax=255)\n",
    "        ax.set_title(f\"batch {i+1} image {j+1}\")\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of moving mean and moving variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=[1.0]\n",
      "beta=[0.0]\n",
      "mean=[0.9649999737739563]\n",
      "var=[36.775001525878906]\n",
      "\n",
      "gamma=[1.0]\n",
      "beta=[0.0]\n",
      "mean=[2.3628499507904053]\n",
      "var=[66.109130859375]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bn_layer = tf.keras.layers.BatchNormalization()\n",
    "bn_layer.build(input_shape=(None, 2, 2, 1))\n",
    "for batch in batches:\n",
    "    bn_layer(batch, training=True)\n",
    "    gamma, beta, mean, var = bn_layer.get_weights()\n",
    "    print(f\"gamma={gamma.tolist()}\\nbeta={beta.tolist()}\\nmean={mean.tolist()}\\nvar={var.tolist()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_running_mean(inpt, running_mean_till_now, momentum=0.99):\n",
    "    batch_mean = np.mean(inpt)\n",
    "    running_mean = momentum * running_mean_till_now + (1 - momentum) * batch_mean\n",
    "    return running_mean\n",
    "\n",
    "def calc_running_var(inpt, running_var_till_now, momentum=0.99):\n",
    "    batch_var = np.var(inpt)\n",
    "    running_var = momentum * running_var_till_now + (1 - momentum) * batch_var\n",
    "    return running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_mean_till_now=0.9650000000000009\n",
      "running_var_till_now=36.775000000000034\n",
      "\n",
      "running_mean_till_now=2.362850000000002\n",
      "running_var_till_now=66.10912500000006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_mean_till_now = 0\n",
    "running_var_till_now = 1\n",
    "for batch in batches:\n",
    "    running_mean_till_now = calc_running_mean(batch, running_mean_till_now)\n",
    "    running_var_till_now = calc_running_var(batch, running_var_till_now)\n",
    "    print(f\"{running_mean_till_now=}\\n{running_var_till_now=}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of inputs to the BatchNormalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 2, 1), dtype=float32, numpy=\n",
       "array([[[[ 0.0919416 ],\n",
       "         [ 1.3791243 ]],\n",
       "\n",
       "        [[-0.07522488],\n",
       "         [-1.3791242 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.15880823],\n",
       "         [-0.42627478]],\n",
       "\n",
       "        [[ 1.5295743 ],\n",
       "         [-1.2788242 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_layer = tf.keras.layers.BatchNormalization()\n",
    "bn_layer.build(input_shape=(None, 2, 2, 1))\n",
    "batch_mod = bn_layer(batches[0], training=True)  # modifier batch after normalization, scale and shift\n",
    "batch_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.09194163],\n",
       "         [ 1.37912439]],\n",
       "\n",
       "        [[-0.07522497],\n",
       "         [-1.37912439]]],\n",
       "\n",
       "\n",
       "       [[[ 0.15880826],\n",
       "         [-0.42627481]],\n",
       "\n",
       "        [[ 1.52957432],\n",
       "         [-1.27882443]]]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_gamma = 1\n",
    "init_beta = 0\n",
    "batch_norm = (batches[0]-batches[0].mean())/np.std(batches[0])\n",
    "batch_mod = batch_norm * init_gamma + init_beta\n",
    "batch_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/batch_norm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
