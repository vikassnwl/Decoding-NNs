{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load an Xception model, pretrained on ImageNet. We exclude the top of\n",
    "the network by setting include_top=False. This excludes the global average pooling\n",
    "layer and the dense output layer. We then add our own global average pooling layer\n",
    "(feeding it the output of the base model), followed by a dense output layer with one\n",
    "unit per class, using the softmax activation function. Finally, we wrap all this in a\n",
    "Keras Model:\n",
    "```py\n",
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "```\n",
    "As explained in Chapter 11, it’s usually a good idea to freeze the weights of the\n",
    "pretrained layers, at least at the beginning of training:\n",
    "```python\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "> Since our model uses the base model’s layers directly, rather than the base_model object itself, setting base_model.trainable=False would have no effect.\n",
    "\n",
    "Finally, we can compile the model and start training:\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)\n",
    "```\n",
    "> If you are running in Colab, make sure the runtime is using a GPU: select Runtime → “Change runtime type”, choose “GPU” in the “Hardware accelerator” drop-down menu, then click Save. It’s possible to train the model without a GPU, but it will be terribly slow (minutes per epoch, as opposed to seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model for a few epochs, its validation accuracy should reach a bit\n",
    "over 80% and then stop improving. This means that the top layers are now pretty\n",
    "well trained, and we are ready to unfreeze some of the base model’s top layers, then\n",
    "continue training. For example, let’s unfreeze layers 56 and above (that’s the start of\n",
    "residual unit 7 out of 14, as you can see if you list the layer names):\n",
    "```python\n",
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True\n",
    "```\n",
    "Don’t forget to compile the model whenever you freeze or unfreeze layers. Also make\n",
    "sure to use a much lower learning rate to avoid damaging the pretrained weights:\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)\n",
    "```\n",
    "This model should reach around 92% accuracy on the test set, in just a few minutes of training (with a GPU). If you tune the hyperparameters, lower the learning rate, and train for quite a bit longer, you should be able to reach 95% to 97%. With that, you can start training amazing image classifiers on your own images and classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
