{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 05:48:35.563463: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-22 05:48:35.568013: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-22 05:48:35.635007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-22 05:48:37.034839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import mlu_tools.utils as mlutils\n",
    "from utils.consts import DATASET_DIR\n",
    "from mlu_tools.plotting import grid_plot\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tVXlXFr4xamxut9bV7GFUQlgm8XiiRQf&export=download\n",
      "To: /home/vikas/Lab/Decoding NNs/datasets/dummy_dataset.zip\n",
      "100%|██████████| 17.5M/17.5M [00:03<00:00, 4.89MB/s]\n"
     ]
    }
   ],
   "source": [
    "# downloading dummy_dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1tVXlXFr4xamxut9bV7GFUQlgm8XiiRQf/view?usp=drive_link\"\n",
    "save_dataset_as = f\"{DATASET_DIR}/dummy_dataset.zip\"\n",
    "mlutils.download(dataset_url, save_dataset_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /home/vikas/Lab/Decoding NNs/datasets/dummy_dataset.zip\n",
      "replace /home/vikas/Lab/Decoding NNs/datasets/dummy_dataset/0/dog (1) (copy).jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# unzip dataset\n",
    "!unzip \"{save_dataset_as}\" -d \"{DATASET_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "dummy_dataset_dir = f\"{DATASET_DIR}/dummy_dataset\"\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "train_generator = datagen.flow_from_directory(dummy_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── dummy_dataset\n",
      "    ├── 0 - 12\n",
      "    ├── 1 - 12\n",
      "    └── 2 - 12\n"
     ]
    }
   ],
   "source": [
    "mlutils.tree(dummy_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "4\n",
      "32\n",
      "4\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for images, labels in train_generator:\n",
    "    print(len(images))\n",
    "    i += 1\n",
    "    if i==5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_generator:\n",
    "    print(len(images))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.keras.preprocessing.image.ImageDataGenerator**\n",
    "\n",
    "- This function creates a pool of all the images from all the subdirectories (subdirectories 0, 1 and 2 in our case).\n",
    "- Each subdirectory is having 12 images in our case so the pool will have 12+12+12=36 images.\n",
    "- By default the batch size is 32 for ImageDataGenerator.flow_from_directory function.\n",
    "- Unlike image_dataset_from_directory, we can iterate over the generator indefinitely.\n",
    "- At each iteration we will get a batch of images of size 32 by default.\n",
    "- We had 36 images in total so at first iteration we got 32 images from the pool leaving 4 images there for the second iteration.\n",
    "- There won't be any duplicate image fetched until we have already fetched all the images from the directory.\n",
    "- As you can see in the loop above that at first iteration 32 images were fetched then remaining 4 images got fetched after that again 32 images got fetches then remaining 4 images got fetched and this will keep going indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
